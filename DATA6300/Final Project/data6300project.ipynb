{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import all the data from the data set, and label them as their own types:  COVID-19, Normal, LUNG Opacity, Viral Pneumonia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (9.2.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.24.2)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: openpyxl in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.1.5)\n",
      "Collecting timm\n",
      "  Downloading timm-1.0.15-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.10.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: et-xmlfile in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openpyxl) (2.0.0)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.17.2-cp311-cp311-macosx_10_13_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyyaml\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-macosx_10_9_x86_64.whl (184 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.6/184.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_10_12_x86_64.whl (436 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.9/436.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.9 in /Users/vanris/Library/Python/3.11/lib/python/site-packages (from huggingface_hub->timm) (23.0)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface_hub->timm) (2.29.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (2022.12.7)\n",
      "Installing collected packages: safetensors, pyyaml, huggingface_hub, torchvision, timm\n",
      "Successfully installed huggingface_hub-0.29.3 pyyaml-6.0.2 safetensors-0.5.3 timm-1.0.15 torchvision-0.17.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pillow numpy tqdm torch openpyxl timm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading COVID: 100%|██████████| 3616/3616 [00:11<00:00, 321.30it/s]\n",
      "Loading Normal: 100%|██████████| 10192/10192 [00:39<00:00, 260.44it/s]\n",
      "Loading Lung_Opacity: 100%|██████████| 6012/6012 [00:20<00:00, 293.40it/s]\n",
      "Loading Viral_Pneumonia: 100%|██████████| 1345/1345 [00:04<00:00, 293.03it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "CLASS_DIRS = {\n",
    "    \"COVID\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/COVID/images\",\n",
    "    \"Normal\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Normal/images\",\n",
    "    \"Lung_Opacity\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Lung_Opacity/images\",\n",
    "    \"Viral_Pneumonia\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Viral Pneumonia/images\"\n",
    "}\n",
    "\n",
    "CLASS_LABELS = {\n",
    "    \"COVID\": 0,\n",
    "    \"Normal\": 1,\n",
    "    \"Lung_Opacity\": 2,\n",
    "    \"Viral_Pneumonia\": 3\n",
    "}\n",
    "\n",
    "def load_class_data(class_dir, label, target_size=(299, 299)):\n",
    "    \"\"\"加载单个类别的图像数据并添加标签\"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in tqdm(os.listdir(class_dir), desc=f\"Loading {label}\"):\n",
    "        if filename.endswith(\".png\"):\n",
    "            image_path = os.path.join(class_dir, filename)\n",
    "            img = Image.open(image_path).convert('L').resize(target_size)\n",
    "            img_array = np.array(img) / 255.0  # 添加归一化\n",
    "            images.append(img_array)\n",
    "            labels.append(CLASS_LABELS[label])\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "def build_full_dataset():\n",
    "    \"\"\"构建完整数据集（带标签）\"\"\"\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    for class_name, class_dir in CLASS_DIRS.items():\n",
    "        images, labels = load_class_data(class_dir, class_name)\n",
    "        all_images.append(images)\n",
    "        all_labels.append(labels)\n",
    "    full_images = np.concatenate(all_images, axis=0)\n",
    "    full_labels = np.concatenate(all_labels, axis=0)\n",
    "    return full_images, full_labels\n",
    "\n",
    "\n",
    "X, y = build_full_dataset()\n",
    "print(\"Final dataset shape:\", X.shape)  # (21165, 299, 299)\n",
    "print(\"Labels shape:\", y.shape)        # (21165,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the dataframe, it's time for deep learning.\n",
    "CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (14815, 1, 299, 299) (14815,)\n",
      "Test shape: (6350, 1, 299, 299) (6350,)\n"
     ]
    }
   ],
   "source": [
    "# (num_samples, height, width)\n",
    "X = X[:, np.newaxis, :, :]  # → (num_samples, 1, height, width)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3, \n",
    "    stratify=y, \n",
    "    random_state=66\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, y_train.shape)  # (16932, 1, 299, 299) (16932,)\n",
    "print(\"Test shape:\", X_test.shape, y_test.shape)     # (6349.5, 1, 299, 299) (4233,)\n",
    "\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,        # \n",
    "    num_workers=8        # \n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False        # \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the CNN using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.set_num_threads(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class COVID_CNN(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(COVID_CNN, self).__init__()\n",
    "        \n",
    "        # conv1\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1,   # input\n",
    "            out_channels=32, # output\n",
    "            kernel_size=3,    # 3x3 kernel\n",
    "            padding=1         # keep size\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  \n",
    "        \n",
    "        # conv2\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        \n",
    "        # fullyconnected layer\n",
    "        self.fc1 = nn.Linear(64 * 74 * 74, 512)  \n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # conv1 → ReLU → pool\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # (batch_size, 32, 149, 149)\n",
    "        \n",
    "        # conv2 → ReLU → pool\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # (batch_size, 64, 74, 74)\n",
    "        \n",
    "        # view → flatten\n",
    "        x = x.view(-1, 64 * 74 * 74)\n",
    "        \n",
    "        # fc1 → ReLU -> Dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVID_CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=350464, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=4, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = COVID_CNN(num_classes=4).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
    "    model.train()  \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        \n",
    "        train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "        \n",
    "        for inputs, labels in train_loader_tqdm:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            \n",
    "            train_loader_tqdm.set_postfix(loss=running_loss/(total/len(labels)), acc=correct/total)\n",
    "        \n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get('MallocStackLogging') == '1':\n",
    "    os.environ['MallocStackLogging'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/463 [00:00<?, ?it/s]Python(20465) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(20491) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(20492) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(20493) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(20494) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(20495) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(20496) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(20497) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(20498) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.8862, Acc: 0.7134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:   0%|          | 0/463 [00:00<?, ?it/s]Python(22820) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(22821) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(22822) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(22823) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(22824) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(22825) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(22826) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(22827) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.4644, Acc: 0.8200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:   0%|          | 0/463 [00:00<?, ?it/s]Python(24932) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(24933) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(24934) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(24935) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(24936) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(24937) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(24938) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(24939) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.3721, Acc: 0.8578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:   0%|          | 0/463 [00:00<?, ?it/s]Python(26999) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(27000) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(27001) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(27002) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(27003) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(27004) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(27005) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(27006) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.3099, Acc: 0.8827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:   0%|          | 0/463 [00:00<?, ?it/s]Python(29109) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(29110) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(29111) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(29112) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(29113) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(29114) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(29115) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(29116) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.2467, Acc: 0.9068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:   0%|          | 0/463 [00:00<?, ?it/s]Python(31248) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(31249) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(31250) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(31251) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(31252) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(31253) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(31254) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(31255) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.1990, Acc: 0.9255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:   0%|          | 0/463 [00:00<?, ?it/s]Python(33391) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(33392) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(33393) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(33394) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(33395) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(33396) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(33397) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(33398) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.1586, Acc: 0.9400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:   0%|          | 0/463 [00:00<?, ?it/s]Python(35612) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(35613) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(35614) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(35615) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(35617) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(35618) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(35619) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(35620) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.1238, Acc: 0.9549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:   0%|          | 0/463 [00:00<?, ?it/s]Python(37886) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(37887) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(37888) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(37889) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(37890) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(37891) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(37892) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(37893) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.0955, Acc: 0.9639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:   0%|          | 0/463 [00:00<?, ?it/s]Python(40085) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(40086) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(40087) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(40088) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(40089) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(40090) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(40091) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(40092) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.0791, Acc: 0.9717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, criterion, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8668\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "       COVID-19       0.92      0.89      0.91      1085\n",
      "         Normal       0.85      0.92      0.88      3058\n",
      "   Lung_Opacity       0.84      0.75      0.79      1804\n",
      "Viral_Pneumonia       0.94      0.92      0.93       403\n",
      "\n",
      "       accuracy                           0.87      6350\n",
      "      macro avg       0.89      0.87      0.88      6350\n",
      "   weighted avg       0.87      0.87      0.87      6350\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  \n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    test_acc = correct / total\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    return test_acc\n",
    "\n",
    "evaluate_model(model, test_loader)\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def get_classification_report(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    print(classification_report(y_true, y_pred, target_names=CLASS_NAMES))\n",
    "\n",
    "CLASS_NAMES = [\"COVID-19\", \"Normal\", \"Lung_Opacity\", \"Viral_Pneumonia\"]\n",
    "get_classification_report(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"covid_cnn_model.pth\")\n",
    "loaded_model = COVID_CNN(num_classes=4).to(device)\n",
    "loaded_model.load_state_dict(torch.load(\"covid_cnn_model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try the classification method we learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_flat = X.reshape(X.shape[0], -1)\n",
    "X = X_flat.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_flat\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X\n",
    "del y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF 未标准化准确率: 0.8577840774864163\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf_raw = rf.predict(X_test)\n",
    "print(\"RF Accuracy:\", accuracy_score(y_test, y_pred_rf_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost 未标准化准确率: 0.6810772501771793\n"
     ]
    }
   ],
   "source": [
    "ada = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "ada.fit(X_train, y_train)\n",
    "y_pred_ada_raw = ada.predict(X_test)\n",
    "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, y_pred_ada_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then it's pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  Train Loss: 2.5041, Train Acc: 37.54%\n",
      "  Val Loss: 2.1244, Val Acc: 44.33%\n",
      "Saved best model with Val Acc: 44.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "  Train Loss: 1.8261, Train Acc: 49.60%\n",
      "  Val Loss: 1.6310, Val Acc: 53.91%\n",
      "Saved best model with Val Acc: 53.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "  Train Loss: 1.4930, Train Acc: 56.25%\n",
      "  Val Loss: 1.3825, Val Acc: 58.60%\n",
      "Saved best model with Val Acc: 58.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "  Train Loss: 1.2962, Train Acc: 61.20%\n",
      "  Val Loss: 1.2146, Val Acc: 63.83%\n",
      "Saved best model with Val Acc: 63.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "  Train Loss: 1.1747, Train Acc: 64.10%\n",
      "  Val Loss: 1.1510, Val Acc: 64.65%\n",
      "Saved best model with Val Acc: 64.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "\n",
    "\n",
    "CLASS_DIRS = {\n",
    "    \"COVID\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/COVID/images\",\n",
    "    \"Normal\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Normal/images\",\n",
    "    \"Lung_Opacity\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Lung_Opacity/images\",\n",
    "    \"Viral_Pneumonia\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Viral Pneumonia/images\"\n",
    "}\n",
    "\n",
    "CLASS_LABELS = {\n",
    "    \"COVID\": 0,\n",
    "    \"Normal\": 1,\n",
    "    \"Lung_Opacity\": 2,\n",
    "    \"Viral_Pneumonia\": 3\n",
    "}\n",
    "\n",
    "\n",
    "class COVIDDataset(Dataset):\n",
    "    def __init__(self, class_dirs, class_labels, transform=None, target_size=(224, 224)):\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "        for class_name, class_dir in class_dirs.items():\n",
    "            for filename in os.listdir(class_dir):\n",
    "                if filename.endswith(\".png\"):\n",
    "                    image_path = os.path.join(class_dir, filename)\n",
    "                    self.images.append(image_path)\n",
    "                    self.labels.append(class_labels[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        \n",
    "        img = Image.open(image_path).convert('L')  \n",
    "        img = img.convert('RGB')  \n",
    "        img = img.resize(self.target_size)  \n",
    "\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "\n",
    "full_dataset = COVIDDataset(CLASS_DIRS, CLASS_LABELS, transform=train_transform, target_size=(224, 224))\n",
    "\n",
    "\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "num_train = int(train_ratio * len(full_dataset))\n",
    "num_val = int(val_ratio * len(full_dataset))\n",
    "num_test = len(full_dataset) - num_train - num_val\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, [num_train, num_val, num_test], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=len(CLASS_LABELS))\n",
    "\n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train_with_progress(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10):\n",
    "    model = model.to(device)\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False)\n",
    "        for inputs, labels in train_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            \n",
    "            train_pbar.set_postfix({\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"Acc\": f\"{100. * train_correct / train_total:.2f}%\"\n",
    "            })\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = 100. * train_correct / train_total\n",
    "\n",
    "        \n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_efficientnet_B0.pth')\n",
    "            print(f\"Saved best model with Val Acc: {best_val_acc:.2f}%\")\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=\"Validating\", leave=False)\n",
    "        for inputs, labels in val_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            val_pbar.set_postfix({\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"Acc\": f\"{100. * val_correct / val_total:.2f}%\"\n",
    "            })\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = 100. * val_correct / val_total\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "train_with_progress(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  Train Loss: 2.0847, Acc: 42.96%\n",
      "  Val Loss: 1.7957, Acc: 48.30%\n",
      "Saved new best model with Acc: 48.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "  Train Loss: 1.5776, Acc: 54.01%\n",
      "  Val Loss: 1.4444, Acc: 57.50%\n",
      "Saved new best model with Acc: 57.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "  Train Loss: 1.3719, Acc: 59.29%\n",
      "  Val Loss: 1.3128, Acc: 61.12%\n",
      "Saved new best model with Acc: 61.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "  Train Loss: 1.2174, Acc: 63.64%\n",
      "  Val Loss: 1.1251, Acc: 65.56%\n",
      "Saved new best model with Acc: 65.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "  Train Loss: 1.1117, Acc: 65.80%\n",
      "  Val Loss: 1.0577, Acc: 68.71%\n",
      "Saved new best model with Acc: 68.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "\n",
    "\n",
    "CLASS_DIRS = {\n",
    "    \"COVID\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/COVID/images\",\n",
    "    \"Normal\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Normal/images\",\n",
    "    \"Lung_Opacity\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Lung_Opacity/images\",\n",
    "    \"Viral_Pneumonia\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Viral Pneumonia/images\"\n",
    "}\n",
    "\n",
    "CLASS_LABELS = {\n",
    "    \"COVID\": 0,\n",
    "    \"Normal\": 1,\n",
    "    \"Lung_Opacity\": 2,\n",
    "    \"Viral_Pneumonia\": 3\n",
    "}\n",
    "\n",
    "\n",
    "class COVIDDataset(Dataset):\n",
    "    def __init__(self, class_dirs, class_labels, transform=None, target_size=(224, 224)):\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "\n",
    "        \n",
    "        for class_name, class_dir in class_dirs.items():\n",
    "            for filename in os.listdir(class_dir):\n",
    "                if filename.endswith(\".png\"):\n",
    "                    image_path = os.path.join(class_dir, filename)\n",
    "                    self.images.append(image_path)\n",
    "                    self.labels.append(class_labels[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        \n",
    "        img = Image.open(image_path).convert('L')  \n",
    "        img = img.convert('RGB')  \n",
    "        img = img.resize(self.target_size)  \n",
    "\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "\n",
    "full_dataset = COVIDDataset(CLASS_DIRS, CLASS_LABELS, transform=train_transform, target_size=(224, 224))\n",
    "\n",
    "\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "num_train = int(train_ratio * len(full_dataset))\n",
    "num_val = int(val_ratio * len(full_dataset))\n",
    "num_test = len(full_dataset) - num_train - num_val\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, [num_train, num_val, num_test], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=len(CLASS_LABELS))\n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "blocks_to_unfreeze = [\n",
    "    model.classifier\n",
    "]\n",
    "for block in blocks_to_unfreeze:\n",
    "    for param in block.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train_with_progress(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=5):\n",
    "    model = model.to(device)\n",
    "    best_val_acc = 0.0\n",
    "    patience = 2\n",
    "    stale_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False)\n",
    "        for inputs, labels in train_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            train_pbar.set_postfix({\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"Acc\": f\"{100.*train_correct/train_total:.2f}%\"\n",
    "            })\n",
    "\n",
    "        \n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss/len(train_loader):.4f}, Acc: {100.*train_correct/train_total:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_efficientnet_B0_v2.pth')\n",
    "            stale_epochs = 0\n",
    "            print(f\"Saved new best model with Acc: {val_acc:.2f}%\")\n",
    "        else:\n",
    "            stale_epochs += 1\n",
    "            if stale_epochs >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=\"Validating\", leave=False)\n",
    "        for inputs, labels in val_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            val_pbar.set_postfix({\"Acc\": f\"{100.*correct/total:.2f}%\"})\n",
    "\n",
    "    return val_loss/len(val_loader), 100.*correct/total\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "train_with_progress(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['efficientnetv2_l', 'efficientnetv2_m', 'efficientnetv2_rw_m', 'efficientnetv2_rw_s', 'efficientnetv2_rw_t', 'efficientnetv2_s', 'efficientnetv2_xl', 'gc_efficientnetv2_rw_t', 'tf_efficientnetv2_b0', 'tf_efficientnetv2_b1', 'tf_efficientnetv2_b2', 'tf_efficientnetv2_b3', 'tf_efficientnetv2_l', 'tf_efficientnetv2_m', 'tf_efficientnetv2_s', 'tf_efficientnetv2_xl']\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "print(timm.list_models('*efficientnetv2*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  Train Loss: 2.1187, Acc: 33.63%\n",
      "  Val Loss: 1.7591, Acc: 41.97%\n",
      "Saved new best model with Acc: 41.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "  Train Loss: 1.5342, Acc: 49.21%\n",
      "  Val Loss: 1.4164, Acc: 52.27%\n",
      "Saved new best model with Acc: 52.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "  Train Loss: 1.2756, Acc: 56.48%\n",
      "  Val Loss: 1.2329, Acc: 58.19%\n",
      "Saved new best model with Acc: 58.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "  Train Loss: 1.1387, Acc: 60.76%\n",
      "  Val Loss: 1.0777, Acc: 61.85%\n",
      "Saved new best model with Acc: 61.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "  Train Loss: 1.0272, Acc: 64.41%\n",
      "  Val Loss: 0.9781, Acc: 63.93%\n",
      "Saved new best model with Acc: 63.93%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "\n",
    "\n",
    "CLASS_DIRS = {\n",
    "    \"COVID\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/COVID/images\",\n",
    "    \"Normal\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Normal/images\",\n",
    "    \"Lung_Opacity\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Lung_Opacity/images\",\n",
    "    \"Viral_Pneumonia\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Viral Pneumonia/images\"\n",
    "}\n",
    "\n",
    "CLASS_LABELS = {\n",
    "    \"COVID\": 0,\n",
    "    \"Normal\": 1,\n",
    "    \"Lung_Opacity\": 2,\n",
    "    \"Viral_Pneumonia\": 3\n",
    "}\n",
    "\n",
    "\n",
    "class COVIDDataset(Dataset):\n",
    "    def __init__(self, class_dirs, class_labels, transform=None, target_size=(224, 224)):\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "\n",
    "        \n",
    "        for class_name, class_dir in class_dirs.items():\n",
    "            for filename in os.listdir(class_dir):\n",
    "                if filename.endswith(\".png\"):\n",
    "                    image_path = os.path.join(class_dir, filename)\n",
    "                    self.images.append(image_path)\n",
    "                    self.labels.append(class_labels[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        \n",
    "        img = Image.open(image_path).convert('L')  \n",
    "        img = img.convert('RGB')  \n",
    "        img = img.resize(self.target_size)  \n",
    "\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "\n",
    "full_dataset = COVIDDataset(CLASS_DIRS, CLASS_LABELS, transform=train_transform, target_size=(224, 224))\n",
    "\n",
    "\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "num_train = int(train_ratio * len(full_dataset))\n",
    "num_val = int(val_ratio * len(full_dataset))\n",
    "num_test = len(full_dataset) - num_train - num_val\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, [num_train, num_val, num_test], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "model = timm.create_model('tf_efficientnetv2_b0', pretrained=True, num_classes=len(CLASS_LABELS))  # B0 版本\n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "blocks_to_unfreeze = [\n",
    "    model.classifier         \n",
    "]\n",
    "for block in blocks_to_unfreeze:\n",
    "    for param in block.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train_with_progress(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=5):\n",
    "    model = model.to(device)\n",
    "    best_val_acc = 0.0\n",
    "    patience = 2\n",
    "    stale_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False)\n",
    "        for inputs, labels in train_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            train_pbar.set_postfix({\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"Acc\": f\"{100.*train_correct/train_total:.2f}%\"\n",
    "            })\n",
    "\n",
    "        \n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss/len(train_loader):.4f}, Acc: {100.*train_correct/train_total:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_efficientnet_v2_B0_v2.pth')\n",
    "            stale_epochs = 0\n",
    "            print(f\"Saved new best model with Acc: {val_acc:.2f}%\")\n",
    "        else:\n",
    "            stale_epochs += 1\n",
    "            if stale_epochs >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=\"Validating\", leave=False)\n",
    "        for inputs, labels in val_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            val_pbar.set_postfix({\"Acc\": f\"{100.*correct/total:.2f}%\"})\n",
    "\n",
    "    return val_loss/len(val_loader), 100.*correct/total\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "train_with_progress(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  Train Loss: 1.2117, Acc: 62.18%\n",
      "  Val Loss: 1.0639, Acc: 66.73%\n",
      "Saved new best model with Acc: 66.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "  Train Loss: 0.9993, Acc: 67.80%\n",
      "  Val Loss: 0.9275, Acc: 69.53%\n",
      "Saved new best model with Acc: 69.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "  Train Loss: 0.8913, Acc: 70.83%\n",
      "  Val Loss: 0.8345, Acc: 72.87%\n",
      "Saved new best model with Acc: 72.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "  Train Loss: 0.8265, Acc: 72.78%\n",
      "  Val Loss: 0.7806, Acc: 74.39%\n",
      "Saved new best model with Acc: 74.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "  Train Loss: 0.7841, Acc: 74.08%\n",
      "  Val Loss: 0.7513, Acc: 75.11%\n",
      "Saved new best model with Acc: 75.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "\n",
    "\n",
    "CLASS_DIRS = {\n",
    "    \"COVID\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/COVID/images\",\n",
    "    \"Normal\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Normal/images\",\n",
    "    \"Lung_Opacity\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Lung_Opacity/images\",\n",
    "    \"Viral_Pneumonia\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Viral Pneumonia/images\"\n",
    "}\n",
    "\n",
    "CLASS_LABELS = {\n",
    "    \"COVID\": 0,\n",
    "    \"Normal\": 1,\n",
    "    \"Lung_Opacity\": 2,\n",
    "    \"Viral_Pneumonia\": 3\n",
    "}\n",
    "\n",
    "\n",
    "class COVIDDataset(Dataset):\n",
    "    def __init__(self, class_dirs, class_labels, transform=None, target_size=(224, 224)):\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "\n",
    "        \n",
    "        for class_name, class_dir in class_dirs.items():\n",
    "            for filename in os.listdir(class_dir):\n",
    "                if filename.endswith(\".png\"):\n",
    "                    image_path = os.path.join(class_dir, filename)\n",
    "                    self.images.append(image_path)\n",
    "                    self.labels.append(class_labels[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        \n",
    "        img = Image.open(image_path).convert('L')  \n",
    "        img = img.convert('RGB')  \n",
    "        img = img.resize(self.target_size)  \n",
    "\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "\n",
    "full_dataset = COVIDDataset(CLASS_DIRS, CLASS_LABELS, transform=train_transform, target_size=(224, 224))\n",
    "\n",
    "\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "num_train = int(train_ratio * len(full_dataset))\n",
    "num_val = int(val_ratio * len(full_dataset))\n",
    "num_test = len(full_dataset) - num_train - num_val\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, [num_train, num_val, num_test], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "model = timm.create_model('mobilevit_s', pretrained=True, num_classes=len(CLASS_LABELS))\n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "blocks_to_unfreeze = [\n",
    "    model.head         \n",
    "]\n",
    "for block in blocks_to_unfreeze:\n",
    "    for param in block.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train_with_progress(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=5):\n",
    "    model = model.to(device)\n",
    "    best_val_acc = 0.0\n",
    "    patience = 2\n",
    "    stale_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False)\n",
    "        for inputs, labels in train_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            train_pbar.set_postfix({\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"Acc\": f\"{100.*train_correct/train_total:.2f}%\"\n",
    "            })\n",
    "\n",
    "        \n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss/len(train_loader):.4f}, Acc: {100.*train_correct/train_total:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_mobilevit_s.pth')\n",
    "            stale_epochs = 0\n",
    "            print(f\"Saved new best model with Acc: {val_acc:.2f}%\")\n",
    "        else:\n",
    "            stale_epochs += 1\n",
    "            if stale_epochs >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=\"Validating\", leave=False)\n",
    "        for inputs, labels in val_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            val_pbar.set_postfix({\"Acc\": f\"{100.*correct/total:.2f}%\"})\n",
    "\n",
    "    return val_loss/len(val_loader), 100.*correct/total\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "train_with_progress(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mobilevitv2_050', 'mobilevitv2_075', 'mobilevitv2_100', 'mobilevitv2_125', 'mobilevitv2_150', 'mobilevitv2_175', 'mobilevitv2_200']\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "print(timm.list_models('*mobilevitv2*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  Train Loss: 1.2355, Acc: 64.01%\n",
      "  Val Loss: 1.1110, Acc: 70.48%\n",
      "Saved new best model with Acc: 70.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "  Train Loss: 1.0374, Acc: 70.95%\n",
      "  Val Loss: 0.9509, Acc: 73.47%\n",
      "Saved new best model with Acc: 73.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "  Train Loss: 0.9184, Acc: 72.92%\n",
      "  Val Loss: 0.8663, Acc: 73.98%\n",
      "Saved new best model with Acc: 73.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "  Train Loss: 0.8449, Acc: 74.29%\n",
      "  Val Loss: 0.8012, Acc: 76.69%\n",
      "Saved new best model with Acc: 76.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "  Train Loss: 0.7955, Acc: 75.15%\n",
      "  Val Loss: 0.7464, Acc: 77.50%\n",
      "Saved new best model with Acc: 77.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "\n",
    "\n",
    "CLASS_DIRS = {\n",
    "    \"COVID\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/COVID/images\",\n",
    "    \"Normal\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Normal/images\",\n",
    "    \"Lung_Opacity\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Lung_Opacity/images\",\n",
    "    \"Viral_Pneumonia\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Viral Pneumonia/images\"\n",
    "}\n",
    "\n",
    "CLASS_LABELS = {\n",
    "    \"COVID\": 0,\n",
    "    \"Normal\": 1,\n",
    "    \"Lung_Opacity\": 2,\n",
    "    \"Viral_Pneumonia\": 3\n",
    "}\n",
    "\n",
    "\n",
    "class COVIDDataset(Dataset):\n",
    "    def __init__(self, class_dirs, class_labels, transform=None, target_size=(224, 224)):\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "\n",
    "        \n",
    "        for class_name, class_dir in class_dirs.items():\n",
    "            for filename in os.listdir(class_dir):\n",
    "                if filename.endswith(\".png\"):\n",
    "                    image_path = os.path.join(class_dir, filename)\n",
    "                    self.images.append(image_path)\n",
    "                    self.labels.append(class_labels[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        \n",
    "        img = Image.open(image_path).convert('L')  \n",
    "        img = img.convert('RGB')  \n",
    "        img = img.resize(self.target_size)  \n",
    "\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "\n",
    "full_dataset = COVIDDataset(CLASS_DIRS, CLASS_LABELS, transform=train_transform, target_size=(224, 224))\n",
    "\n",
    "\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "num_train = int(train_ratio * len(full_dataset))\n",
    "num_val = int(val_ratio * len(full_dataset))\n",
    "num_test = len(full_dataset) - num_train - num_val\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, [num_train, num_val, num_test], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "model = timm.create_model('mobilevitv2_100', pretrained=True, num_classes=len(CLASS_LABELS))\n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "blocks_to_unfreeze = [\n",
    "    model.head         \n",
    "]\n",
    "for block in blocks_to_unfreeze:\n",
    "    for param in block.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train_with_progress(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=5):\n",
    "    model = model.to(device)\n",
    "    best_val_acc = 0.0\n",
    "    patience = 2\n",
    "    stale_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False)\n",
    "        for inputs, labels in train_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            train_pbar.set_postfix({\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"Acc\": f\"{100.*train_correct/train_total:.2f}%\"\n",
    "            })\n",
    "\n",
    "        \n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss/len(train_loader):.4f}, Acc: {100.*train_correct/train_total:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_mobilevit_s_v2.pth')\n",
    "            stale_epochs = 0\n",
    "            print(f\"Saved new best model with Acc: {val_acc:.2f}%\")\n",
    "        else:\n",
    "            stale_epochs += 1\n",
    "            if stale_epochs >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=\"Validating\", leave=False)\n",
    "        for inputs, labels in val_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            val_pbar.set_postfix({\"Acc\": f\"{100.*correct/total:.2f}%\"})\n",
    "\n",
    "    return val_loss/len(val_loader), 100.*correct/total\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "train_with_progress(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['edgenext_base', 'edgenext_small', 'edgenext_small_rw', 'edgenext_x_small', 'edgenext_xx_small']\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "print(timm.list_models('*edgenext*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  Train Loss: 1.0194, Acc: 58.42%\n",
      "  Val Loss: 0.8288, Acc: 66.64%\n",
      "Saved new best model with Acc: 66.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "  Train Loss: 0.7562, Acc: 70.36%\n",
      "  Val Loss: 0.6878, Acc: 74.64%\n",
      "Saved new best model with Acc: 74.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "  Train Loss: 0.6576, Acc: 75.19%\n",
      "  Val Loss: 0.6215, Acc: 76.91%\n",
      "Saved new best model with Acc: 76.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "  Train Loss: 0.6044, Acc: 77.86%\n",
      "  Val Loss: 0.5778, Acc: 78.89%\n",
      "Saved new best model with Acc: 78.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "  Train Loss: 0.5668, Acc: 79.37%\n",
      "  Val Loss: 0.5415, Acc: 80.18%\n",
      "Saved new best model with Acc: 80.18%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "\n",
    "\n",
    "CLASS_DIRS = {\n",
    "    \"COVID\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/COVID/images\",\n",
    "    \"Normal\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Normal/images\",\n",
    "    \"Lung_Opacity\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Lung_Opacity/images\",\n",
    "    \"Viral_Pneumonia\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Viral Pneumonia/images\"\n",
    "}\n",
    "\n",
    "CLASS_LABELS = {\n",
    "    \"COVID\": 0,\n",
    "    \"Normal\": 1,\n",
    "    \"Lung_Opacity\": 2,\n",
    "    \"Viral_Pneumonia\": 3\n",
    "}\n",
    "\n",
    "\n",
    "class COVIDDataset(Dataset):\n",
    "    def __init__(self, class_dirs, class_labels, transform=None, target_size=(224, 224)):\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "\n",
    "        \n",
    "        for class_name, class_dir in class_dirs.items():\n",
    "            for filename in os.listdir(class_dir):\n",
    "                if filename.endswith(\".png\"):\n",
    "                    image_path = os.path.join(class_dir, filename)\n",
    "                    self.images.append(image_path)\n",
    "                    self.labels.append(class_labels[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        \n",
    "        img = Image.open(image_path).convert('L')  \n",
    "        img = img.convert('RGB')  \n",
    "        img = img.resize(self.target_size)  \n",
    "\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "\n",
    "full_dataset = COVIDDataset(CLASS_DIRS, CLASS_LABELS, transform=train_transform, target_size=(224, 224))\n",
    "\n",
    "\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "num_train = int(train_ratio * len(full_dataset))\n",
    "num_val = int(val_ratio * len(full_dataset))\n",
    "num_test = len(full_dataset) - num_train - num_val\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, [num_train, num_val, num_test], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "model = timm.create_model('edgenext_small', pretrained=True, num_classes=len(CLASS_LABELS))\n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "blocks_to_unfreeze = [\n",
    "    model.head         \n",
    "]\n",
    "for block in blocks_to_unfreeze:\n",
    "    for param in block.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train_with_progress(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=5):\n",
    "    model = model.to(device)\n",
    "    best_val_acc = 0.0\n",
    "    patience = 2\n",
    "    stale_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False)\n",
    "        for inputs, labels in train_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            train_pbar.set_postfix({\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"Acc\": f\"{100.*train_correct/train_total:.2f}%\"\n",
    "            })\n",
    "\n",
    "        \n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss/len(train_loader):.4f}, Acc: {100.*train_correct/train_total:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'edgenext_small.pth')\n",
    "            stale_epochs = 0\n",
    "            print(f\"Saved new best model with Acc: {val_acc:.2f}%\")\n",
    "        else:\n",
    "            stale_epochs += 1\n",
    "            if stale_epochs >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=\"Validating\", leave=False)\n",
    "        for inputs, labels in val_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            val_pbar.set_postfix({\"Acc\": f\"{100.*correct/total:.2f}%\"})\n",
    "\n",
    "    return val_loss/len(val_loader), 100.*correct/total\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "train_with_progress(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unfreeze last three layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  Train Loss: 2.5078, Train Acc: 39.23%\n",
      "  Val Loss: 1.9839, Val Acc: 47.13%\n",
      "Saved best model with Val Acc: 47.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "  Train Loss: 1.7434, Train Acc: 52.15%\n",
      "  Val Loss: 1.5075, Val Acc: 56.90%\n",
      "Saved best model with Val Acc: 56.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "  Train Loss: 1.4336, Train Acc: 58.91%\n",
      "  Val Loss: 1.2974, Val Acc: 62.67%\n",
      "Saved best model with Val Acc: 62.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "  Train Loss: 1.2757, Train Acc: 62.94%\n",
      "  Val Loss: 1.1733, Val Acc: 65.63%\n",
      "Saved best model with Val Acc: 65.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "  Train Loss: 1.1560, Train Acc: 65.68%\n",
      "  Val Loss: 1.0609, Val Acc: 67.30%\n",
      "Saved best model with Val Acc: 67.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "\n",
    "\n",
    "CLASS_DIRS = {\n",
    "    \"COVID\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/COVID/images\",\n",
    "    \"Normal\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Normal/images\",\n",
    "    \"Lung_Opacity\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Lung_Opacity/images\",\n",
    "    \"Viral_Pneumonia\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Viral Pneumonia/images\"\n",
    "}\n",
    "\n",
    "CLASS_LABELS = {\n",
    "    \"COVID\": 0,\n",
    "    \"Normal\": 1,\n",
    "    \"Lung_Opacity\": 2,\n",
    "    \"Viral_Pneumonia\": 3\n",
    "}\n",
    "\n",
    "\n",
    "class COVIDDataset(Dataset):\n",
    "    def __init__(self, class_dirs, class_labels, transform=None, target_size=(224, 224)):\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "\n",
    "        \n",
    "        for class_name, class_dir in class_dirs.items():\n",
    "            for filename in os.listdir(class_dir):\n",
    "                if filename.endswith(\".png\"):\n",
    "                    image_path = os.path.join(class_dir, filename)\n",
    "                    self.images.append(image_path)\n",
    "                    self.labels.append(class_labels[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        \n",
    "        img = Image.open(image_path).convert('L')  \n",
    "        img = img.convert('RGB')  \n",
    "        img = img.resize(self.target_size)  \n",
    "\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "\n",
    "full_dataset = COVIDDataset(CLASS_DIRS, CLASS_LABELS, transform=train_transform, target_size=(224, 224))\n",
    "\n",
    "\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "num_train = int(train_ratio * len(full_dataset))\n",
    "num_val = int(val_ratio * len(full_dataset))\n",
    "num_test = len(full_dataset) - num_train - num_val\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, [num_train, num_val, num_test], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=len(CLASS_LABELS))\n",
    "\n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "layers = list(model.children())\n",
    "\n",
    "\n",
    "for layer in layers[-3:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train_with_progress(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10):\n",
    "    model = model.to(device)\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False)\n",
    "        for inputs, labels in train_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            \n",
    "            train_pbar.set_postfix({\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"Acc\": f\"{100. * train_correct / train_total:.2f}%\"\n",
    "            })\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = 100. * train_correct / train_total\n",
    "\n",
    "        \n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'last_three_layers_best_efficientnet_B0.pth')\n",
    "            print(f\"Saved best model with Val Acc: {best_val_acc:.2f}%\")\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=\"Validating\", leave=False)\n",
    "        for inputs, labels in val_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            val_pbar.set_postfix({\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"Acc\": f\"{100. * val_correct / val_total:.2f}%\"\n",
    "            })\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = 100. * val_correct / val_total\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "train_with_progress(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  Train Loss: 1.9864, Train Acc: 36.65%\n",
      "  Val Loss: 1.5761, Val Acc: 46.60%\n",
      "Saved best model with Val Acc: 46.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "  Train Loss: 1.4118, Train Acc: 51.97%\n",
      "  Val Loss: 1.2558, Val Acc: 56.58%\n",
      "Saved best model with Val Acc: 56.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "  Train Loss: 1.1995, Train Acc: 58.03%\n",
      "  Val Loss: 1.1079, Val Acc: 60.24%\n",
      "Saved best model with Val Acc: 60.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "  Train Loss: 1.0601, Train Acc: 62.28%\n",
      "  Val Loss: 0.9936, Val Acc: 64.65%\n",
      "Saved best model with Val Acc: 64.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "  Train Loss: 0.9809, Train Acc: 65.45%\n",
      "  Val Loss: 0.9179, Val Acc: 66.35%\n",
      "Saved best model with Val Acc: 66.35%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "\n",
    "\n",
    "CLASS_DIRS = {\n",
    "    \"COVID\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/COVID/images\",\n",
    "    \"Normal\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Normal/images\",\n",
    "    \"Lung_Opacity\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Lung_Opacity/images\",\n",
    "    \"Viral_Pneumonia\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Viral Pneumonia/images\"\n",
    "}\n",
    "\n",
    "CLASS_LABELS = {\n",
    "    \"COVID\": 0,\n",
    "    \"Normal\": 1,\n",
    "    \"Lung_Opacity\": 2,\n",
    "    \"Viral_Pneumonia\": 3\n",
    "}\n",
    "\n",
    "\n",
    "class COVIDDataset(Dataset):\n",
    "    def __init__(self, class_dirs, class_labels, transform=None, target_size=(224, 224)):\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "\n",
    "        \n",
    "        for class_name, class_dir in class_dirs.items():\n",
    "            for filename in os.listdir(class_dir):\n",
    "                if filename.endswith(\".png\"):\n",
    "                    image_path = os.path.join(class_dir, filename)\n",
    "                    self.images.append(image_path)\n",
    "                    self.labels.append(class_labels[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        \n",
    "        img = Image.open(image_path).convert('L')  \n",
    "        img = img.convert('RGB')  \n",
    "        img = img.resize(self.target_size)  \n",
    "\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "\n",
    "full_dataset = COVIDDataset(CLASS_DIRS, CLASS_LABELS, transform=train_transform, target_size=(224, 224))\n",
    "\n",
    "\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "num_train = int(train_ratio * len(full_dataset))\n",
    "num_val = int(val_ratio * len(full_dataset))\n",
    "num_test = len(full_dataset) - num_train - num_val\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, [num_train, num_val, num_test], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "model = timm.create_model('tf_efficientnetv2_b0', pretrained=True, num_classes=len(CLASS_LABELS))\n",
    "\n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "layers = list(model.children())\n",
    "\n",
    "\n",
    "for layer in layers[-3:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train_with_progress(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10):\n",
    "    model = model.to(device)\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False)\n",
    "        for inputs, labels in train_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            \n",
    "            train_pbar.set_postfix({\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"Acc\": f\"{100. * train_correct / train_total:.2f}%\"\n",
    "            })\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = 100. * train_correct / train_total\n",
    "\n",
    "        \n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'last_three_layers_best_efficientnet_V2_B0.pth')\n",
    "            print(f\"Saved best model with Val Acc: {best_val_acc:.2f}%\")\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=\"Validating\", leave=False)\n",
    "        for inputs, labels in val_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            val_pbar.set_postfix({\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"Acc\": f\"{100. * val_correct / val_total:.2f}%\"\n",
    "            })\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = 100. * val_correct / val_total\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "train_with_progress(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  Train Loss: 0.5039, Acc: 86.12%\n",
      "  Val Loss: 0.2500, Acc: 92.12%\n",
      "Saved new best model with Acc: 92.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "  Train Loss: 0.2356, Acc: 92.47%\n",
      "  Val Loss: 0.1785, Acc: 94.11%\n",
      "Saved new best model with Acc: 94.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "  Train Loss: 0.1838, Acc: 93.86%\n",
      "  Val Loss: 0.1505, Acc: 95.15%\n",
      "Saved new best model with Acc: 95.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "  Train Loss: 0.1554, Acc: 94.56%\n",
      "  Val Loss: 0.1596, Acc: 94.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "  Train Loss: 0.1304, Acc: 95.52%\n",
      "  Val Loss: 0.1528, Acc: 94.68%\n",
      "Early stopping at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "\n",
    "\n",
    "CLASS_DIRS = {\n",
    "    \"COVID\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/COVID/images\",\n",
    "    \"Normal\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Normal/images\",\n",
    "    \"Lung_Opacity\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Lung_Opacity/images\",\n",
    "    \"Viral_Pneumonia\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Viral Pneumonia/images\"\n",
    "}\n",
    "\n",
    "CLASS_LABELS = {\n",
    "    \"COVID\": 0,\n",
    "    \"Normal\": 1,\n",
    "    \"Lung_Opacity\": 2,\n",
    "    \"Viral_Pneumonia\": 3\n",
    "}\n",
    "\n",
    "\n",
    "class COVIDDataset(Dataset):\n",
    "    def __init__(self, class_dirs, class_labels, transform=None, target_size=(224, 224)):\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "\n",
    "        \n",
    "        for class_name, class_dir in class_dirs.items():\n",
    "            for filename in os.listdir(class_dir):\n",
    "                if filename.endswith(\".png\"):\n",
    "                    image_path = os.path.join(class_dir, filename)\n",
    "                    self.images.append(image_path)\n",
    "                    self.labels.append(class_labels[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        \n",
    "        img = Image.open(image_path).convert('L')  \n",
    "        img = img.convert('RGB')  \n",
    "        img = img.resize(self.target_size)  \n",
    "\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "\n",
    "full_dataset = COVIDDataset(CLASS_DIRS, CLASS_LABELS, transform=train_transform, target_size=(224, 224))\n",
    "\n",
    "\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "num_train = int(train_ratio * len(full_dataset))\n",
    "num_val = int(val_ratio * len(full_dataset))\n",
    "num_test = len(full_dataset) - num_train - num_val\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, [num_train, num_val, num_test], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "model = timm.create_model('mobilevit_s', pretrained=True, num_classes=len(CLASS_LABELS))\n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "layers = list(model.children())\n",
    "\n",
    "\n",
    "for layer in layers[-3:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train_with_progress(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=5):\n",
    "    model = model.to(device)\n",
    "    best_val_acc = 0.0\n",
    "    patience = 2\n",
    "    stale_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False)\n",
    "        for inputs, labels in train_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            train_pbar.set_postfix({\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"Acc\": f\"{100.*train_correct/train_total:.2f}%\"\n",
    "            })\n",
    "\n",
    "        \n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss/len(train_loader):.4f}, Acc: {100.*train_correct/train_total:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'last_three_layers_best_mobilevit_s.pth')\n",
    "            stale_epochs = 0\n",
    "            print(f\"Saved new best model with Acc: {val_acc:.2f}%\")\n",
    "        else:\n",
    "            stale_epochs += 1\n",
    "            if stale_epochs >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=\"Validating\", leave=False)\n",
    "        for inputs, labels in val_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            val_pbar.set_postfix({\"Acc\": f\"{100.*correct/total:.2f}%\"})\n",
    "\n",
    "    return val_loss/len(val_loader), 100.*correct/total\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "train_with_progress(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  Train Loss: 0.5060, Acc: 84.52%\n",
      "  Val Loss: 0.2326, Acc: 93.07%\n",
      "Saved new best model with Acc: 93.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "  Train Loss: 0.2257, Acc: 92.37%\n",
      "  Val Loss: 0.1806, Acc: 93.86%\n",
      "Saved new best model with Acc: 93.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "  Train Loss: 0.1795, Acc: 93.84%\n",
      "  Val Loss: 0.1583, Acc: 94.71%\n",
      "Saved new best model with Acc: 94.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "  Train Loss: 0.1532, Acc: 94.59%\n",
      "  Val Loss: 0.1514, Acc: 94.83%\n",
      "Saved new best model with Acc: 94.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "  Train Loss: 0.1287, Acc: 95.38%\n",
      "  Val Loss: 0.1380, Acc: 95.21%\n",
      "Saved new best model with Acc: 95.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "\n",
    "\n",
    "CLASS_DIRS = {\n",
    "    \"COVID\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/COVID/images\",\n",
    "    \"Normal\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Normal/images\",\n",
    "    \"Lung_Opacity\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Lung_Opacity/images\",\n",
    "    \"Viral_Pneumonia\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Viral Pneumonia/images\"\n",
    "}\n",
    "\n",
    "CLASS_LABELS = {\n",
    "    \"COVID\": 0,\n",
    "    \"Normal\": 1,\n",
    "    \"Lung_Opacity\": 2,\n",
    "    \"Viral_Pneumonia\": 3\n",
    "}\n",
    "\n",
    "\n",
    "class COVIDDataset(Dataset):\n",
    "    def __init__(self, class_dirs, class_labels, transform=None, target_size=(224, 224)):\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "\n",
    "        \n",
    "        for class_name, class_dir in class_dirs.items():\n",
    "            for filename in os.listdir(class_dir):\n",
    "                if filename.endswith(\".png\"):\n",
    "                    image_path = os.path.join(class_dir, filename)\n",
    "                    self.images.append(image_path)\n",
    "                    self.labels.append(class_labels[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        \n",
    "        img = Image.open(image_path).convert('L')  \n",
    "        img = img.convert('RGB')  \n",
    "        img = img.resize(self.target_size)  \n",
    "\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "\n",
    "full_dataset = COVIDDataset(CLASS_DIRS, CLASS_LABELS, transform=train_transform, target_size=(224, 224))\n",
    "\n",
    "\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "num_train = int(train_ratio * len(full_dataset))\n",
    "num_val = int(val_ratio * len(full_dataset))\n",
    "num_test = len(full_dataset) - num_train - num_val\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, [num_train, num_val, num_test], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "model = timm.create_model('mobilevitv2_100', pretrained=True, num_classes=len(CLASS_LABELS))\n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "layers = list(model.children())\n",
    "\n",
    "\n",
    "for layer in layers[-3:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train_with_progress(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=5):\n",
    "    model = model.to(device)\n",
    "    best_val_acc = 0.0\n",
    "    patience = 2\n",
    "    stale_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False)\n",
    "        for inputs, labels in train_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            train_pbar.set_postfix({\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"Acc\": f\"{100.*train_correct/train_total:.2f}%\"\n",
    "            })\n",
    "\n",
    "        \n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss/len(train_loader):.4f}, Acc: {100.*train_correct/train_total:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'last_three_layers_best_mobilevit_s_v2.pth')\n",
    "            stale_epochs = 0\n",
    "            print(f\"Saved new best model with Acc: {val_acc:.2f}%\")\n",
    "        else:\n",
    "            stale_epochs += 1\n",
    "            if stale_epochs >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=\"Validating\", leave=False)\n",
    "        for inputs, labels in val_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            val_pbar.set_postfix({\"Acc\": f\"{100.*correct/total:.2f}%\"})\n",
    "\n",
    "    return val_loss/len(val_loader), 100.*correct/total\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "train_with_progress(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  Train Loss: 0.3569, Acc: 87.18%\n",
      "  Val Loss: 0.2010, Acc: 93.10%\n",
      "Saved new best model with Acc: 93.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "  Train Loss: 0.1851, Acc: 93.71%\n",
      "  Val Loss: 0.1996, Acc: 92.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "  Train Loss: 0.1534, Acc: 94.47%\n",
      "  Val Loss: 0.1656, Acc: 94.45%\n",
      "Saved new best model with Acc: 94.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "  Train Loss: 0.1265, Acc: 95.63%\n",
      "  Val Loss: 0.1466, Acc: 94.90%\n",
      "Saved new best model with Acc: 94.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "  Train Loss: 0.1038, Acc: 96.30%\n",
      "  Val Loss: 0.2052, Acc: 93.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "\n",
    "\n",
    "CLASS_DIRS = {\n",
    "    \"COVID\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/COVID/images\",\n",
    "    \"Normal\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Normal/images\",\n",
    "    \"Lung_Opacity\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Lung_Opacity/images\",\n",
    "    \"Viral_Pneumonia\": \"/Users/vanris/Documents/UG-DS6300/Project/Dataset/COVID-19_Radiography_Dataset/Viral Pneumonia/images\"\n",
    "}\n",
    "\n",
    "CLASS_LABELS = {\n",
    "    \"COVID\": 0,\n",
    "    \"Normal\": 1,\n",
    "    \"Lung_Opacity\": 2,\n",
    "    \"Viral_Pneumonia\": 3\n",
    "}\n",
    "\n",
    "\n",
    "class COVIDDataset(Dataset):\n",
    "    def __init__(self, class_dirs, class_labels, transform=None, target_size=(224, 224)):\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "\n",
    "        \n",
    "        for class_name, class_dir in class_dirs.items():\n",
    "            for filename in os.listdir(class_dir):\n",
    "                if filename.endswith(\".png\"):\n",
    "                    image_path = os.path.join(class_dir, filename)\n",
    "                    self.images.append(image_path)\n",
    "                    self.labels.append(class_labels[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        \n",
    "        img = Image.open(image_path).convert('L')  \n",
    "        img = img.convert('RGB')  \n",
    "        img = img.resize(self.target_size)  \n",
    "\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "\n",
    "full_dataset = COVIDDataset(CLASS_DIRS, CLASS_LABELS, transform=train_transform, target_size=(224, 224))\n",
    "\n",
    "\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "num_train = int(train_ratio * len(full_dataset))\n",
    "num_val = int(val_ratio * len(full_dataset))\n",
    "num_test = len(full_dataset) - num_train - num_val\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, [num_train, num_val, num_test], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "model = timm.create_model('edgenext_small', pretrained=True, num_classes=len(CLASS_LABELS))\n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "layers = list(model.children())\n",
    "\n",
    "\n",
    "for layer in layers[-3:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train_with_progress(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=5):\n",
    "    model = model.to(device)\n",
    "    best_val_acc = 0.0\n",
    "    patience = 2\n",
    "    stale_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False)\n",
    "        for inputs, labels in train_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            train_pbar.set_postfix({\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"Acc\": f\"{100.*train_correct/train_total:.2f}%\"\n",
    "            })\n",
    "\n",
    "        \n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss/len(train_loader):.4f}, Acc: {100.*train_correct/train_total:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'last_three_layers_edgenext_small.pth')\n",
    "            stale_epochs = 0\n",
    "            print(f\"Saved new best model with Acc: {val_acc:.2f}%\")\n",
    "        else:\n",
    "            stale_epochs += 1\n",
    "            if stale_epochs >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=\"Validating\", leave=False)\n",
    "        for inputs, labels in val_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            val_pbar.set_postfix({\"Acc\": f\"{100.*correct/total:.2f}%\"})\n",
    "\n",
    "    return val_loss/len(val_loader), 100.*correct/total\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "train_with_progress(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc = 100. * test_correct / test_total\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading models and do the testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = timm.create_model('efficientnet_b0', pretrained=True, num_classes=len(CLASS_LABELS))\n",
    "model_2 = timm.create_model('tf_efficientnetv2_b0', pretrained=True, num_classes=len(CLASS_LABELS))\n",
    "model_3 = timm.create_model('mobilevit_s', pretrained=True, num_classes=len(CLASS_LABELS))\n",
    "model_4 = timm.create_model('mobilevitv2_100', pretrained=True, num_classes=len(CLASS_LABELS))\n",
    "model_5 = timm.create_model('edgenext_small', pretrained=True, num_classes=len(CLASS_LABELS))\n",
    "\n",
    "model_13 = timm.create_model('efficientnet_b0', pretrained=True, num_classes=len(CLASS_LABELS))\n",
    "model_23 = timm.create_model('tf_efficientnetv2_b0', pretrained=True, num_classes=len(CLASS_LABELS))\n",
    "model_33 = timm.create_model('mobilevit_s', pretrained=True, num_classes=len(CLASS_LABELS))\n",
    "model_43 = timm.create_model('mobilevitv2_100', pretrained=True, num_classes=len(CLASS_LABELS))\n",
    "model_53 = timm.create_model('edgenext_small', pretrained=True, num_classes=len(CLASS_LABELS))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_1.load_state_dict(torch.load('/Users/vanris/Documents/UG-DS6300/Project/best_efficientnet_B0_v2.pth'))\n",
    "model_2.load_state_dict(torch.load('/Users/vanris/Documents/UG-DS6300/Project/best_efficientnet_v2_B0_v2.pth'))\n",
    "model_3.load_state_dict(torch.load('/Users/vanris/Documents/UG-DS6300/Project/best_mobilevit_s.pth'))\n",
    "model_4.load_state_dict(torch.load('/Users/vanris/Documents/UG-DS6300/Project/best_mobilevit_s_v2.pth'))\n",
    "model_5.load_state_dict(torch.load('/Users/vanris/Documents/UG-DS6300/Project/edgenext_small.pth'))\n",
    "\n",
    "model_13.load_state_dict(torch.load('/Users/vanris/Documents/UG-DS6300/Project/last_three_layers_best_efficientnet_B0.pth'))\n",
    "model_23.load_state_dict(torch.load('/Users/vanris/Documents/UG-DS6300/Project/last_three_layers_best_efficientnet_V2_B0.pth'))\n",
    "model_33.load_state_dict(torch.load('/Users/vanris/Documents/UG-DS6300/Project/last_three_layers_best_mobilevit_s.pth'))\n",
    "model_43.load_state_dict(torch.load('/Users/vanris/Documents/UG-DS6300/Project/last_three_layers_best_mobilevit_s_v2.pth'))\n",
    "model_53.load_state_dict(torch.load('/Users/vanris/Documents/UG-DS6300/Project/last_three_layers_edgenext_small.pth'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.0572, Test Acc: 67.41%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test(model_1, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.0041, Test Acc: 63.00%\n"
     ]
    }
   ],
   "source": [
    "test(model_2, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.7707, Test Acc: 73.58%\n"
     ]
    }
   ],
   "source": [
    "test(model_3, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.7543, Test Acc: 77.36%\n"
     ]
    }
   ],
   "source": [
    "test(model_4, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.5454, Test Acc: 80.35%\n"
     ]
    }
   ],
   "source": [
    "test(model_5, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.1475, Test Acc: 64.48%\n"
     ]
    }
   ],
   "source": [
    "test(model_13, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.9140, Test Acc: 67.73%\n"
     ]
    }
   ],
   "source": [
    "test(model_23, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1574, Test Acc: 95.18%\n"
     ]
    }
   ],
   "source": [
    "test(model_33, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1403, Test Acc: 95.28%\n"
     ]
    }
   ],
   "source": [
    "test(model_43, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1385, Test Acc: 95.28%\n"
     ]
    }
   ],
   "source": [
    "test(model_53, test_loader, criterion, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
